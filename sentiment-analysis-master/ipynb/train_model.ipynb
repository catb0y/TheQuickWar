{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import os.path\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import History\n",
    "from keras.utils import normalize\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re as re\n",
    "from random import randint\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.path.expanduser('~')\n",
    "DATA_DIR = os.path.join(HOME_DIR, '.ipublia', 'data')\n",
    "\n",
    "SETTINGS = {\n",
    "    'en': {\n",
    "        'dataset': 'aclImdb',\n",
    "        'dataset_dir': os.path.join(DATA_DIR, 'aclImdb'),\n",
    "        'max-number-pos': 25000,\n",
    "        'max-number-neg': 25000,\n",
    "        'test-size': 0.5,\n",
    "        'version': '1.0.1'\n",
    "    },\n",
    "    'de': {\n",
    "        'dataset': 'filmstarts',\n",
    "        'dataset_dir': os.path.join(DATA_DIR, 'filmstarts'),\n",
    "        'max-number-pos': 25000,\n",
    "        'max-number-neg': 15500,\n",
    "        'test-size': 0.33,\n",
    "        'version': '1.0.1'\n",
    "    },\n",
    "    'fr': {\n",
    "        'dataset': 'allocine',\n",
    "        'dataset_dir': os.path.join(DATA_DIR, 'allocine'),\n",
    "        'max-number-pos': 25000,\n",
    "        'max-number-neg': 25000,\n",
    "        'test-size': 0.5,\n",
    "        'version': '1.0.1'\n",
    "    },\n",
    "    'it': {\n",
    "        'dataset': 'mymovies',\n",
    "        'dataset_dir': os.path.join(DATA_DIR, 'mymovies'),\n",
    "        'max-number-pos': 25000,\n",
    "        'max-number-neg': 25000,\n",
    "        'test-size': 0.5,\n",
    "        'version': '1.0.1'\n",
    "    }\n",
    "}\n",
    "\n",
    "LANG = 'fr'\n",
    "CONFIG = SETTINGS[LANG]\n",
    "\n",
    "DATASET = CONFIG['dataset']\n",
    "DATASET_DIR = CONFIG['dataset_dir']\n",
    "MAX_NUMBER_POS = CONFIG['max-number-pos']\n",
    "MAX_NUMBER_NEG = CONFIG['max-number-neg']\n",
    "TEST_SIZE = CONFIG['test-size']\n",
    "MODEL_NAME = 'sentiment-analysis'\n",
    "MODEL_VERSION = CONFIG['version']\n",
    "\n",
    "MODEL_FILE = os.path.join(DATA_DIR, MODEL_NAME, 'model_' + LANG + '_' + MODEL_VERSION + '.h5')\n",
    "TOKENIZER_FILE = os.path.join(DATA_DIR, MODEL_NAME, 'tokenizer_' + LANG + '_' + MODEL_VERSION + '.pickle')\n",
    "HISTORY_FILE = os.path.join(DATA_DIR, MODEL_NAME, 'history_' + LANG + '_' + MODEL_VERSION + '.pickle')\n",
    "EMBEDDING_FILE = os.path.join(DATA_DIR, 'facebookresearch', 'wiki.' + LANG + '.vec')\n",
    "\n",
    "MAX_WORDS = None\n",
    "MIN_TEXT_LENGTH = 3\n",
    "MAX_TEXT_LENGTH = 400\n",
    "\n",
    "print('LANG: {}'.format(LANG))\n",
    "print('EMBEDDING_FILE: {}\\nMODEL_FILE: {}'.format(EMBEDDING_FILE, MODEL_FILE))\n",
    "print('CONFIG', CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embedding(file, max_words=None):    \n",
    "    embedding_index = {}\n",
    "    model = KeyedVectors.load_word2vec_format(file, limit=max_words)\n",
    "    \n",
    "    for word in model.vocab:\n",
    "        embedding_index[word] = model[word]\n",
    "        \n",
    "    embedding_dimension = 300 #len(next (iter (embedding_index.values())))\n",
    "    return (embedding_index, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<br />', '', text)\n",
    "    text = re.sub('(\\n|\\r|\\t)+', ' ', text)\n",
    "    text = re.sub('ß', 'ss', text)\n",
    "    text = re.sub('’', \"'\", text)\n",
    "    text = re.sub('[^a-zA-Z0-9.!?,;:\\-\\' äàâæçéèêîïíìöôóòœüûüúùÿ]+', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path,\n",
    "              max_number_pos=100,\n",
    "              max_number_neg=100,\n",
    "              min_words=None,\n",
    "              max_words=None,\n",
    "              clean=False,\n",
    "              shuffle=False):\n",
    "    \n",
    "    def load(path, y_val, max_number):\n",
    "        \n",
    "        x = np.array([])\n",
    "        y = np.array([])\n",
    "        \n",
    "        files = [str(os.path.join(path, f)) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "        loaded_count = 0\n",
    "        \n",
    "        for file in files:\n",
    "            \n",
    "            if loaded_count == max_number:\n",
    "                break\n",
    "                \n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                text = f.readline()\n",
    "                if clean:\n",
    "                    text = clean_text(text)\n",
    "                \n",
    "                splitted = text.split(' ')\n",
    "                if min_words and len(splitted) < min_words:\n",
    "                    continue\n",
    "\n",
    "                if max_words:\n",
    "                    text = ' '.join(splitted[:max_words])\n",
    "\n",
    "                x = np.append(x, text)\n",
    "                y = np.append(y, y_val)\n",
    "                loaded_count += 1 \n",
    "                \n",
    "            if loaded_count % 1000 == 0:\n",
    "                print('  {} items loaded...'.format(loaded_count))\n",
    "        \n",
    "        return (x, y)\n",
    "\n",
    "    print('Loading data from {}'.format(dataset_path))\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    \n",
    "    if DATASET == 'aclImdb':\n",
    "        print('Using train/test source data structure.')\n",
    "        print('Loading {} pos items...'.format(max_number_pos))\n",
    "        x_pos, y_pos = load(os.path.join(dataset_path, 'train', 'pos'), 1, max_number_pos)\n",
    "        x = np.append(x, x_pos)\n",
    "        y = np.append(y, y_pos)\n",
    "        \n",
    "        x_pos, y_pos = load(os.path.join(dataset_path, 'test', 'pos'), 1, max_number_pos - len(x_pos))\n",
    "        x = np.append(x, x_pos)\n",
    "        y = np.append(y, y_pos)\n",
    "        \n",
    "        print('Loading {} neg items...'.format(max_number_neg))\n",
    "        x_neg, y_neg = load(os.path.join(dataset_path, 'train', 'neg'), 0, max_number_neg)\n",
    "        x = np.append(x, x_neg)\n",
    "        y = np.append(y, y_neg)\n",
    "        \n",
    "        x_neg, y_neg = load(os.path.join(dataset_path, 'test', 'neg'), 0, max_number_neg - len(x_neg))\n",
    "        x = np.append(x, x_neg)\n",
    "        y = np.append(y, y_neg)\n",
    "        \n",
    "    else:\n",
    "        print('Using pos/neg source data structure.')\n",
    "        print('Loading {} pos items...'.format(max_number_pos))\n",
    "        x_pos, y_pos = load(os.path.join(dataset_path, 'pos'), 1, max_number_pos)\n",
    "        x = np.append(x, x_pos)\n",
    "        y = np.append(y, y_pos)\n",
    "\n",
    "        print('Loading {} neg items...'.format(max_number_neg))\n",
    "        x_neg, y_neg = load(os.path.join(dataset_path, 'neg'), 0, max_number_neg)\n",
    "        x = np.append(x, x_neg)\n",
    "        y = np.append(y, y_neg)\n",
    "    \n",
    "    print('Loaded {} items.'.format(len(x)))\n",
    "    \n",
    "    if shuffle:\n",
    "        print('Shuffling items...')\n",
    "        p = np.random.permutation(len(x))\n",
    "        x = x[p]\n",
    "        y = y[p]\n",
    "        \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y) = load_data(DATASET_DIR,\n",
    "                   max_number_pos=MAX_NUMBER_POS,\n",
    "                   max_number_neg=MAX_NUMBER_NEG,\n",
    "                   min_words=MIN_TEXT_LENGTH,\n",
    "                   max_words=MAX_TEXT_LENGTH,\n",
    "                   clean=True,\n",
    "                   shuffle=True)\n",
    "\n",
    "print('Loaded {} items.'.format(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Splitting into train and test sets ...')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=0)\n",
    "\n",
    "del x\n",
    "del y\n",
    "\n",
    "print('Shape x_train: {}, y_train: {}'.format(x_train.shape, y_train.shape))\n",
    "print('      x_test: {}, y_test: {}'.format(x_test.shape, y_test.shape))\n",
    "\n",
    "print('Creating tokenizer ...')\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('    Found {} unique tokens.'.format(len(word_index)))\n",
    "\n",
    "print('Vectorizing sequence data ...')\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "print('Padding sequence data ...')\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_TEXT_LENGTH)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "print('Normalizing sequence data ...')\n",
    "#x_train = normalize(x_train.astype(np.float32))\n",
    "#x_test = normalize(x_test.astype(np.float32))\n",
    "x_train = x_train.astype(np.float64)\n",
    "x_test = x_test.astype(np.float64)\n",
    "# print(x_train[0:1][0:10])\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "for i in range(0, 5):\n",
    "    j = np.random.randint(0, len(x_train))\n",
    "    print(x_train[j][-10:], '\\nrating:', y_train[j], '  index:', j, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(embedding_index, embedding_dimension) = load_word_embedding(EMBEDDING_FILE, MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dimension))\n",
    "matches = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        matches += 1\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Length of embedding_matrix: {}, matches with embedding: {}, ratio: {:.4f}'.format(\n",
    "    len(embedding_matrix), matches, matches / len(embedding_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(len(tokenizer.word_index) + 1,\n",
    "              embedding_dimension,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_TEXT_LENGTH,\n",
    "              trainable=False))\n",
    "\n",
    "model.add(LSTM(8, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1)\n",
    "\n",
    "loss, acc = model.evaluate(\n",
    "    x_test, y_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test acc:', acc)\n",
    "print('Accuracy: {:.4f}'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model, Tokenizer and History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_FILE)\n",
    "# Store the tokenizer. The model can't be reused without it.\n",
    "with open(TOKENIZER_FILE, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(HISTORY_FILE, 'wb') as handle:\n",
    "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model, Tokenizer and History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_FILE)\n",
    "with open(TOKENIZER_FILE, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open(HISTORY_FILE, 'rb') as handle:\n",
    "    history = History()\n",
    "    history.history = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train, x_test, y_test) = load_data(TRAIN_AND_TEST_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(a):\n",
    "    count = [0, 0]\n",
    "    for i in range(len(a)):\n",
    "        if(a[i] == 1):\n",
    "            count[0] += 1\n",
    "        else:\n",
    "            count[1] += 1\n",
    "    return count\n",
    "\n",
    "print(len(y_train), len(y_test))\n",
    "print('pos/neg', count(y_train), count(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = lambda x: \"%.4f\" % x\n",
    "\n",
    "for i in range(0, 9):\n",
    "    sample_index = randint(0, len(x_test))\n",
    "    reviews = [x_test[sample_index]]\n",
    "    sequences = tokenizer.texts_to_sequences(reviews)\n",
    "    padded_reviews = pad_sequences(sequences, maxlen=MAX_TEXT_LENGTH)\n",
    "    pred = model.predict(np.array(padded_reviews))[0][0]\n",
    "    \n",
    "    print(sample_index, ':', round(pred) == y_test[sample_index], float_formatter(pred), y_test[sample_index], reviews[0][0:500])\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    'Habe den Film gestern in der Cinelady-Vorstellung gesehen und war enttäuscht. Es wird extrem oberflächlich eine dem Thema nicht gerecht werdende Story ohne jeden Höhepunkt erzählt.',\n",
    "    'Dieser Film ist allen Menschen zu empfehlen, die sich für die Tragik der Liebe und für tiefgehende Dialoge und Gefühle interessieren.',\n",
    "    'Dieser Film ist vom Anfang bis am Ende spannend! Die Schauspieler sind super!',\n",
    "    'Dieser Film ist vom Anfang bis am Ende langweilig! Die Schauspieler sind mässig bis schlecht!',\n",
    "    'Ein super Film, überhaupt nicht wertend sondern extrem informativ und spannend wie unsere Politik funktioniert!',\n",
    "    'Eine sehr einseitige Dokumentation. Nicht empfehlenswert!',\n",
    "    'Wunder ist ein Wunderbarer Film mit einem grandiosen Darsteller-Cast bei dem kein Auge trocken bleibt.Köstlich für alle Star Wars Fans sind die liebevollen Anspielung auf Star Wars die im Film vorkommen.Für alle Homeland Fans ist in einem kurz auftritt der Star aus dessen Serie Mandy Patinkin zu sehen.Sowie ist Wunder für den Oscar 2018 als bestes Make-up nominiert.Dafür gibts von Mir 4.1/2 Sterne von 5.',\n",
    "    'Kann ausnahmsweise der Cinema-Kritik absolut recht geben. Alle Figuren unglaubwürdig, übertrieben und jedes Klischee verwendet. Aber heute reicht es scheinbar, wenn es laut ist und viele Leute völlig unnötig niedergemetzelt werden - Logik braucht es dazu nicht. Schade!!',\n",
    "    'Das Rundumpacket ist super. Leider ist der Monitor mit dem GSync Modul nochmals knappe 100.- teurer, sodass ich selber nicht von dem FreeSync Modul profitieren kann, da ich eine NVidia Grafikkarte besitze. Die 240 Hz sind echt spürbar im Shooterbereich. Die Farben kommen trotz dem TN Panel gut rüber. Für den Durchschnittspieler sind die 240 Hz überflüssig. Aber wenn man sich im eSports aufhält, können diese echt etwas ausmachen.',\n",
    "    'Mit diesen BT-Kopfhörern bin ich nur mittelmässig zufrieden. Die BOSE SoundSport Wireless sind meine 3. BT-Kopfhörer in nur 2 Jahren. Bislang war ich in der Preisklasse um die 100 CHF unterwegs. Nachdem BT In-Ears nun schon seit 3 Jahren auf dem Markt sind, wollte ich etwas mehr investieren und habe mich anhand anderer Bewertungen (leider) für die SoundSport entschieden. Zu den Produkteigenschaften: Der Klang ist super, die Bässe total präzise. Die maximale Lautstärke hingegen könnte noch mehr sein. Manchmal braucht man das halt. Das Gehäuse ist nicht vollständig geschlossen, da kommen Umgebungsgeräusche durch, denn die Silikon-Pads sind nicht ganz dicht. Überhaupt die Silikon-Pads! Normalerweise habe ich bei den Grösse M, bei den SoundSport aber L. Immerhin: sie sitzen bombenfest und drücken konstruktionsbedingt kein bisschen. Aber: für Menschen mit grossen Ohren könnte L zu klein sein. Die Verbindungsqualität ist exzellent. Die SoundSport connecten super schnell und die Verbindung ist äusserst stabil. Kein Ruckeln in der Übertragung, selbst wenn das Natel in der Hosentasche steckt. Ebenfalls super ist die Ladegeschwindigkeit mit ca. 1.5 Stunden wenn die Akkus komplett leer sind. Die Akkus halten bei mir aber nicht 8h, sondern nur ca. 4h, Lautstärke liegt meist bei 80%. Ein nettes Feature ist die Ladezustandsansage beim Anschalten. Blöd ist nur, dass die Restlaufzeit nach der Warnmeldung \"Batterieladezustand niedrig\" nur noch 5-10 Minuten beträgt. Was jetzt noch bleibt sind die Bedienelemente. An-/Ausschalter befinden sich direkt am rechten Ear-Plug. Laut/Leise/Start/Stop und Mikro sind im Bedienelement am Kabel untergebracht, aber die Druckpunkte sind unterirdisch! Vor/Zurück gibts nicht (oder funzt nicht mit meinem HTC one), man muss in dem Fall immer das Natel/MP3-Player rausholen. Alles in einem einzigen Bedienelement unterzubringen, inkl. Skip-Funktion, ist doch längst Standard.',\n",
    "    'Etwas gross aber mit spitzen Klang und Tragekomfort. Verbindet schnell und stabil via Bluetooth. Ich hatte schon einige bluetooth in-ohr Kopfhörer aber dieser Bose ist bei weitem der Beste.'\n",
    "]\n",
    "\n",
    "reviews = [\n",
    "    'J\\'aime ce film. Les acteurs jouent vraiment bien!',\n",
    "    'Je n\\'aime pas ce film. Les acteurs jouent vraiment mal!'\n",
    "]\n",
    "\n",
    "val = [\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    1\n",
    "]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "padded_reviews = pad_sequences(sequences, maxlen=MAX_TEXT_LENGTH)\n",
    "preds = model.predict(np.array(padded_reviews))\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    print('{}, {:.4f}, {}, {}'.format(round(preds[i][0]) == val[i], preds[i][0], val[i], reviews[i][0:1000]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
